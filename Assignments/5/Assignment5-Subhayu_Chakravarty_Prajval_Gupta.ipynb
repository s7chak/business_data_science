{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>HW5 - Business Data Science (MIS382N)</center>\n",
    "### <center>Submitted by Prajval Gupta and Subhayu Chakravarty</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem walks us through a problem discussed in detail in the Multi-level regression bookwritten  by  A.  Gelman  and  J. Hill. NYC  has  a  program  known  as  stop-and-frisk. Relying  on  a60’s era ruling, the law allows an officer to search someone without arrest, and without probable cause,  if  the  officer  believes  s/he  might  be  in  danger  because  of  a  hidden  weapon. Much  has been  written  about  this,  as  it  has  come  under  significant  scrutiny  for  being  discriminative  and allowing (even encouraging) racial profiling.  You can read a summary of it at this Wikipedia page: https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City. The data you will download contain information about the number of traffic stops, reported per precinct (75 total precincts), along with the ethnicity as reported by the police officer.  These data have kept only three ethnicities:  white, black and hispanic.  The data set also has data on arrest rates in the previous year, broken down by four types of crimes; and the total population levels perprecinct per ethnic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod.families.family import Poisson\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Download and load the data in the file NYCstopandfrisk.dat, uploaded to Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Read\n",
    "df = pd.read_csv(\"NYC_stop_and_frisk.dat\",skiprows=5, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  What fraction of the total stops correspond to “white/back/hispanic”?  What fraction of thepopulation corresponds to “white/black/hispanic”?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eth</th>\n",
       "      <th>stops</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.531297</td>\n",
       "      <td>0.275469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.339545</td>\n",
       "      <td>0.255741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.129158</td>\n",
       "      <td>0.468789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eth     stops       pop\n",
       "0    1  0.531297  0.275469\n",
       "1    2  0.339545  0.255741\n",
       "2    3  0.129158  0.468789"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fractions = df.groupby([\"eth\"])[\"stops\",\"pop\"].sum().apply(lambda x: x/x.sum()).reset_index()\n",
    "fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of total stops corresponding to eth 1 is 0.5312966063004109\n",
      "The fraction of total stops corresponding to eth 2 is 0.3395449703241516\n",
      "The fraction of total stops corresponding to eth 3 is 0.12915842337543754\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fractions.stops)):\n",
    "    print(\"The fraction of total stops corresponding to eth {} is {}\".format(fractions.eth[i],fractions.stops[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of total population corresponding to eth 1 is 0.27546911368656585\n",
      "The fraction of total population corresponding to eth 2 is 0.2557414035070107\n",
      "The fraction of total population corresponding to eth 3 is 0.46878948280642346\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(fractions[\"pop\"])):\n",
    "    print(\"The fraction of total population corresponding to eth {} is {}\".format(fractions.eth[j],fractions[\"pop\"][j])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Use a Poisson regression to model the number of stops,  controlling for ethnicity and using the number of past arrests as an exposure input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stops</th>\n",
       "      <th>pop</th>\n",
       "      <th>past.arrests</th>\n",
       "      <th>precinct</th>\n",
       "      <th>eth</th>\n",
       "      <th>crime</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>1720</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>1720</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>1720</td>\n",
       "      <td>599</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>1720</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>1368</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stops   pop  past.arrests  precinct  eth  crime  black  hispanic  white\n",
       "0     75  1720           191         1    1      1      1         0      0\n",
       "1     36  1720            57         1    1      2      1         0      0\n",
       "2     74  1720           599         1    1      3      1         0      0\n",
       "3     17  1720           133         1    1      4      1         0      0\n",
       "4     37  1368            62         1    2      1      0         1      0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.get_dummies(df.eth)\n",
    "df2 = pd.concat([df,df1],axis=1).rename({1:\"black\",2:\"hispanic\",3:\"white\"},axis=1)\n",
    "df2[\"past.arrests\"].replace(0,1,inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2.iloc[:,0]\n",
    "x = df2.iloc[:,[1,-3,-2]]\n",
    "past_arrests = df2.iloc[:,2].values\n",
    "x = add_constant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                  stops   No. Observations:                  900\n",
      "Model:                            GLM   Df Residuals:                      896\n",
      "Model Family:                 Poisson   Df Model:                            3\n",
      "Link Function:                    log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -94260.\n",
      "Date:                Thu, 17 Oct 2019   Deviance:                   1.8318e+05\n",
      "Time:                        21:17:33   Pearson chi2:                 2.79e+05\n",
      "No. Iterations:                     6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.8002      0.009    -89.279      0.000      -0.818      -0.783\n",
      "pop         8.991e-07    8.1e-08     11.102      0.000     7.4e-07    1.06e-06\n",
      "black          0.1729      0.009     20.055      0.000       0.156       0.190\n",
      "hispanic       0.2463      0.009     27.005      0.000       0.228       0.264\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "family = Poisson()\n",
    "glm = GLM(y,x,family=family,exposure=past_arrests)\n",
    "results = glm.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  According to the output of your model, what fraction fewer or more stops does each ethnicityhave with respect to the others, in proportion to arrest rates of the previous year?  Note that you can just pick a baseline ethnicity and just compare everything to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline ethinicity is 'white'\n",
      "The black ethinicty is more likely to be stopped than white by : 1.1887696375463817\n",
      "The hispanic ethinicty is more likely to be stopped than white by : 1.279264428693029\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline ethinicity is 'white'\")\n",
    "print(\"The black ethinicty is more likely to be stopped than white by : {}\".format(np.exp(results.params['black'])))\n",
    "print(\"The hispanic ethinicty is more likely to be stopped than white by : {}\".format(np.exp(results.params['hispanic'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Next, add the 75 precincts, and again solve the Poisson regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.get_dummies(df.precinct)\n",
    "df4 = pd.concat([df2,df3],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df4.drop([\"stops\",\"past.arrests\",\"eth\",\"white\",\"crime\",\"precinct\"],axis=1)\n",
    "y1 = df4.iloc[:,0]\n",
    "x1 = add_constant(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                  stops   No. Observations:                  900\n",
      "Model:                            GLM   Df Residuals:                      822\n",
      "Model Family:                 Poisson   Df Model:                           77\n",
      "Link Function:                    log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -72943.\n",
      "Date:                Thu, 17 Oct 2019   Deviance:                   1.4055e+05\n",
      "Time:                        21:22:47   Pearson chi2:                 2.15e+05\n",
      "No. Iterations:                     7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.1267      0.013    -83.858      0.000      -1.153      -1.100\n",
      "pop         3.796e-06   1.42e-07     26.751      0.000    3.52e-06    4.07e-06\n",
      "black          0.5776      0.012     49.881      0.000       0.555       0.600\n",
      "hispanic       0.5650      0.011     50.422      0.000       0.543       0.587\n",
      "1             -0.8198      0.051    -16.193      0.000      -0.919      -0.721\n",
      "2             -0.9640      0.053    -18.088      0.000      -1.068      -0.860\n",
      "3             -0.2856      0.025    -11.350      0.000      -0.335      -0.236\n",
      "4              0.3517      0.027     13.163      0.000       0.299       0.404\n",
      "5             -0.5726      0.025    -22.779      0.000      -0.622      -0.523\n",
      "6              0.3253      0.028     11.619      0.000       0.270       0.380\n",
      "7             -0.6513      0.039    -16.709      0.000      -0.728      -0.575\n",
      "8             -1.2168      0.026    -47.703      0.000      -1.267      -1.167\n",
      "9             -0.3646      0.059     -6.216      0.000      -0.480      -0.250\n",
      "10            -0.4119      0.030    -13.946      0.000      -0.470      -0.354\n",
      "11            -0.3711      0.035    -10.748      0.000      -0.439      -0.303\n",
      "12             0.2773      0.033      8.285      0.000       0.212       0.343\n",
      "13             0.1324      0.021      6.219      0.000       0.091       0.174\n",
      "14            -0.2927      0.029    -10.045      0.000      -0.350      -0.236\n",
      "15             0.1631      0.021      7.899      0.000       0.123       0.204\n",
      "16            -0.0714      0.032     -2.223      0.026      -0.134      -0.008\n",
      "17            -0.9770      0.032    -30.192      0.000      -1.040      -0.914\n",
      "18            -0.7907      0.021    -38.319      0.000      -0.831      -0.750\n",
      "19            -0.7125      0.028    -25.640      0.000      -0.767      -0.658\n",
      "20            -0.9744      0.025    -38.673      0.000      -1.024      -0.925\n",
      "21            -0.5954      0.025    -23.781      0.000      -0.644      -0.546\n",
      "22             0.2529      0.016     15.805      0.000       0.222       0.284\n",
      "23            -0.2824      0.021    -13.517      0.000      -0.323      -0.241\n",
      "24             0.3951      0.020     20.153      0.000       0.357       0.434\n",
      "25            -0.1614      0.017     -9.298      0.000      -0.195      -0.127\n",
      "26            -1.1731      0.026    -45.883      0.000      -1.223      -1.123\n",
      "27             1.0064      0.022     44.937      0.000       0.962       1.050\n",
      "28            -1.7968      0.033    -54.088      0.000      -1.862      -1.732\n",
      "29             0.0545      0.019      2.812      0.005       0.017       0.093\n",
      "30            -0.3932      0.022    -18.216      0.000      -0.436      -0.351\n",
      "31             0.7561      0.024     32.095      0.000       0.710       0.802\n",
      "32             0.5001      0.031     16.207      0.000       0.440       0.561\n",
      "33             0.1051      0.019      5.676      0.000       0.069       0.141\n",
      "34             0.6227      0.019     32.782      0.000       0.585       0.660\n",
      "35            -0.1304      0.037     -3.509      0.000      -0.203      -0.058\n",
      "36             0.5926      0.029     20.217      0.000       0.535       0.650\n",
      "37             0.5050      0.031     16.034      0.000       0.443       0.567\n",
      "38             0.8273      0.024     35.171      0.000       0.781       0.873\n",
      "39            -0.8550      0.026    -33.348      0.000      -0.905      -0.805\n",
      "40             0.5917      0.034     17.371      0.000       0.525       0.658\n",
      "41             1.0478      0.023     46.118      0.000       1.003       1.092\n",
      "42             0.0281      0.020      1.414      0.157      -0.011       0.067\n",
      "43            -0.5798      0.026    -22.643      0.000      -0.630      -0.530\n",
      "44            -0.0984      0.025     -3.996      0.000      -0.147      -0.050\n",
      "45            -0.2312      0.019    -12.018      0.000      -0.269      -0.194\n",
      "46            -0.4282      0.018    -23.821      0.000      -0.463      -0.393\n",
      "47             0.2732      0.032      8.658      0.000       0.211       0.335\n",
      "48            -0.6983      0.026    -26.838      0.000      -0.749      -0.647\n",
      "49             0.2532      0.031      8.170      0.000       0.192       0.314\n",
      "50            -0.1345      0.019     -7.266      0.000      -0.171      -0.098\n",
      "51            -0.6865      0.029    -23.748      0.000      -0.743      -0.630\n",
      "52            -0.4976      0.022    -22.774      0.000      -0.540      -0.455\n",
      "53            -0.2494      0.029     -8.674      0.000      -0.306      -0.193\n",
      "54            -0.4534      0.032    -14.374      0.000      -0.515      -0.392\n",
      "55            -0.4708      0.030    -15.947      0.000      -0.529      -0.413\n",
      "56             0.0596      0.042      1.431      0.153      -0.022       0.141\n",
      "57             0.6918      0.034     20.271      0.000       0.625       0.759\n",
      "58             0.6517      0.019     33.436      0.000       0.614       0.690\n",
      "59             0.1911      0.029      6.500      0.000       0.133       0.249\n",
      "60            -0.1765      0.019     -9.387      0.000      -0.213      -0.140\n",
      "61             0.3249      0.025     13.098      0.000       0.276       0.374\n",
      "62             0.0833      0.026      3.248      0.001       0.033       0.134\n",
      "63             0.4089      0.028     14.511      0.000       0.354       0.464\n",
      "64             0.8119      0.027     29.955      0.000       0.759       0.865\n",
      "65             1.0122      0.021     47.987      0.000       0.971       1.054\n",
      "66             1.1257      0.019     60.558      0.000       1.089       1.162\n",
      "67             0.2549      0.020     12.455      0.000       0.215       0.295\n",
      "68             1.2826      0.030     42.720      0.000       1.224       1.341\n",
      "69             0.8352      0.028     29.768      0.000       0.780       0.890\n",
      "70            -0.1462      0.022     -6.593      0.000      -0.190      -0.103\n",
      "71             0.5215      0.019     28.072      0.000       0.485       0.558\n",
      "72             0.4754      0.017     27.772      0.000       0.442       0.509\n",
      "73             0.0594      0.017      3.557      0.000       0.027       0.092\n",
      "74             0.0528      0.028      1.862      0.063      -0.003       0.108\n",
      "75             0.6840      0.055     12.341      0.000       0.575       0.793\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "family = Poisson()\n",
    "glm = GLM(y1,x1,family=family,exposure=past_arrests)\n",
    "results1 = glm.fit()\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Now, controlling for precincts, according to your model, what fraction fewer or more stopsdoes  each  ethnicity  have  with  respect  to  the  others,  in  proportion  to  arrest  rates  of  the previous year?  (Again, just report with respect to a chosen ethnicity as a baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Again the Baseline ethinicity is 'white'\n",
      "The black ethinicty is more likely to be stopped than white by : 1.7818306375027073\n",
      "The hispanic ethinicty is more likely to be stopped than white by : 1.7595067462463552\n"
     ]
    }
   ],
   "source": [
    "print(\"Again the Baseline ethinicity is 'white'\")\n",
    "print(\"The black ethinicty is more likely to be stopped than white by : {}\".format(np.exp(results1.params['black'])))\n",
    "print(\"The hispanic ethinicty is more likely to be stopped than white by : {}\".format(np.exp(results1.params['hispanic'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comp</th>\n",
       "      <th>Height</th>\n",
       "      <th>Points</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Comp  Height  Points  Salary\n",
       "0   9.0    76.0    27.0     0.0\n",
       "1   7.0    78.0    39.0     0.0\n",
       "2   9.0    76.0    39.0     0.0\n",
       "3   9.0    74.0    39.0     0.0\n",
       "4   9.0    74.0    26.0     0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('nba_cc_fake_data.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9413"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows with Salary = 0 :\n",
    "df[df.Salary==0].Salary.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Least Squares Regression is not a good model:\n",
    "Explain why linear regression is not appropriate, given the nature of the data.\n",
    "\n",
    "\n",
    "Answer: <br>\n",
    "This is because a majority of the labels are zeroes(9,413 out of 10,000) and when predicting salary, the model tries to fit a line between the zeroes and the rows with non-zero salaries. The lowest non-zero salary is very high compared to zero and thus we get non-zero predictions using least-squares regression. Thus it is not a good idea to fit Linear regression to a problem with such a variation in labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prediction using Least Squares Regression\n",
    "\n",
    "Try least squares regression, anyway. How well do you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:3].values\n",
    "Y=df.iloc[:,3].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 86344.59732927,  19121.40852818,  11914.96574179, ...,\n",
       "       -22231.93667264, 127528.22085956, 118773.94590382])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17787434482059772"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As expected the R2 Score is quite low ~ 0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Composite Model\n",
    "\n",
    "You will next build a composite model. You will first predict the probability that a player actually makes it to the NBA at all, and then you will build a model to predict the salary of a player, conditioned on the fact of making it to the NBA.<br>\n",
    "– Build a model that predicts the probability of making it to the NBA.<br>\n",
    "– Do a train-test split of 8000/2000 points, train your best model on the training set, and\n",
    "compute the AUC on the test set.<br>\n",
    "– Now, build a model to predict the salary. Note that you may wish to consider a non-\n",
    "linear transformation of your data. What is your R2 score on the test set?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Building classification model to determine if player goes to NBA or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_nba with 0,1 instead of salary\n",
    "Y_nba=Y.copy()\n",
    "Y_nba[np.where(Y>0)]=1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y_nba,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_auc(clf, X_train=X_train, X_test=X_test, Y_train=Y_train, Y_test=Y_test):\n",
    "    clf.fit(X_train, Y_train)\n",
    "    y_pred=clf.predict_proba(X_test)[:, 1]\n",
    "    auc_score=roc_auc_score(Y_test,y_pred)\n",
    "    return round(auc_score,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for Logistic Regression on whole X,Y as training set:  0.9395\n",
      "AUC Score for Logistic Regression on train_X and AUC on test set:  0.9427\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC Score for Logistic Regression on whole X,Y as training set: \",predict_auc(LogisticRegression(C=10), X,X,Y_nba,Y_nba))\n",
    "print(\"AUC Score for Logistic Regression on train_X and AUC on test set: \",predict_auc(LogisticRegression(C=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying with standardized features\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for Logistic Regresion on whole X,Y as training set:  0.9418\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC Score for Logistic Regresion on whole X,Y as training set: \",predict_auc(LogisticRegression(C=10), X_train_std, X_test_std, Y_train, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Finding Best model on 80/20 split of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split of 8000/2000 points\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y_nba,test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for DecisionTreeClassifier:  0.925\n",
      "AUC Score for RandomForestClassifier:  0.9278\n",
      "AUC Score for GradientBoostingClassifier:  0.9383\n",
      "AUC Score for AdaBoostClassifier:  0.924\n",
      "AUC Score for XGBoost:  0.9389\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC Score for DecisionTreeClassifier: \",predict_auc(DecisionTreeClassifier(max_depth=5), X_train, X_test, Y_train, Y_test))\n",
    "print(\"AUC Score for RandomForestClassifier: \",predict_auc(RandomForestClassifier(n_estimators=8, max_depth=5), X_train, X_test, Y_train, Y_test))\n",
    "print(\"AUC Score for GradientBoostingClassifier: \",predict_auc(GradientBoostingClassifier(n_estimators=30, max_depth=4), X_train, X_test, Y_train, Y_test))\n",
    "print(\"AUC Score for AdaBoostClassifier: \",predict_auc(AdaBoostClassifier(n_estimators=100), X_train, X_test, Y_train, Y_test))\n",
    "print(\"AUC Score for XGBoost: \",predict_auc(XGBClassifier(n_estimators=100), X_train, X_test, Y_train, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for DecisionTreeClassifier:  0.925\n",
      "AUC Score for RandomForestClassifier:  0.9283\n",
      "AUC Score for GradientBoostingClassifier:  0.9383\n",
      "AUC Score for AdaBoostClassifier:  0.924\n",
      "AUC Score for XGBoost:  0.9389\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC Score for DecisionTreeClassifier: \",predict_auc(DecisionTreeClassifier(max_depth=5), X_train_std, X_test_std, Y_train, Y_test))\n",
    "print(\"AUC Score for RandomForestClassifier: \",predict_auc(RandomForestClassifier(n_estimators=8, max_depth=5), X_train_std, X_test_std, Y_train, Y_test))\n",
    "print(\"AUC Score for GradientBoostingClassifier: \",predict_auc(GradientBoostingClassifier(n_estimators=30, max_depth=4), X_train_std, X_test_std, Y_train, Y_test))\n",
    "print(\"AUC Score for AdaBoostClassifier: \",predict_auc(AdaBoostClassifier(n_estimators=100), X_train_std, X_test_std, Y_train, Y_test))\n",
    "print(\"AUC Score for XGBoost: \",predict_auc(XGBClassifier(n_estimators=100), X_train_std, X_test_std, Y_train, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'booster': 'gbtree', 'learning_rate': 0.1, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 0.5, 'n_estimators': 80}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9373771622842255"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuning XGBoost Classifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "xgparameters={  'max_delta_step':[0],\n",
    "                'n_estimators':[80],\n",
    "                'booster':['gbtree'],\n",
    "                'min_child_weight':[0.5],\n",
    "                'max_depth':[3],\n",
    "                'learning_rate':[0.1]\n",
    "             }\n",
    "\n",
    "clf=GridSearchCV(XGBClassifier(random_state=7),xgparameters,cv=5)\n",
    "clf.fit(X_train,Y_train)\n",
    "print(clf.best_params_)\n",
    "y_pred=clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "roc_auc_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for XGBoost:  0.9386\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC Score for XGBoost: \",predict_auc(XGBClassifier(n_estimators=80, max_depth=3), X_train, X_test, Y_train, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Model to predict salary conditioned on going to NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear regression model conditioned on probability of going to NBA\n",
    "# We took the probability of going to NBA as a feature: Column 4\n",
    "clf=XGBClassifier()\n",
    "clf.fit(X,Y_nba)\n",
    "y_pred=clf.predict_proba(X)\n",
    "new_X=np.concatenate((X,y_pred), axis=1)\n",
    "new_X=new_X[:,[0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4887458866200912"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(new_X,Y)\n",
    "salary=lr.predict(new_X)\n",
    "r2_score(Y,salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To cross-verify this we took the rows from main dataframe where Salary is not 0 and found an r2_score similar to ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_salary = df[df['Salary']>0].iloc[:,:3].values\n",
    "Y_salary = df[df['Salary']>0].iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5685697969800619"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_salary,Y_salary)\n",
    "salary=lr.predict(X_salary)\n",
    "r2_score(Y_salary,salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predict for given player\n",
    "\n",
    "Compute the expected NBA salary of a high school basketball player who is 6’ 6” tall, is\n",
    "averaging 46 points per game, and is playing in the second most competitive league (comp =\n",
    "9), according to your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_X=[9.0, 78.0, 46.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09216729], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(predict_X)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The given player has a 0.09 probability of not going for NBA, hence salary prediction will be 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
