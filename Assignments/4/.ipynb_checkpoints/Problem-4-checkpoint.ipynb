{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF Files for the ICML 2019 Papers\n",
    "url = \"http://proceedings.mlr.press/v97/\"\n",
    "folder_location = r'./BDS'\n",
    "if not os.path.exists(folder_location):\n",
    "    os.mkdir(folder_location)\n",
    "\n",
    "response = requests.get(url)\n",
    "soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "for link in soup.select(\"a[href$='.pdf']\"):\n",
    "    filename = os.path.join(folder_location,link['href'].split('/')[-1]) # Naming the file with pdf name\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(requests.get(urljoin(url,link['href'])).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PdfMiner:\n",
    "\n",
    "#     def __init__(self, file_path):\n",
    "#         self.file_path = file_path\n",
    "        \n",
    "#     def convert_pdf_to_txt(self):\n",
    "#         resource_manager = PDFResourceManager()\n",
    "#         ret_str = StringIO()\n",
    "#         device = TextConverter(resource_manager, ret_str, codec=\"utf-8\", laparams=LAParams())\n",
    "#         f = open(self.file_path, 'rb')\n",
    "#         interpreter = PDFPageInterpreter(resource_manager, device)\n",
    "#         for page in PDFPage.get_pages(fp, set(), maxpages=0, password=\"\",caching=True, check_extractable=True):\n",
    "#             interpreter.process_page(page)\n",
    "#         f.close()\n",
    "#         device.close()\n",
    "#         str = ret_str.getvalue()\n",
    "#         ret_str.close()\n",
    "#         return str\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     data = []\n",
    "#     for foldername,subfolders,files in os.walk(r\"./BDS\"):\n",
    "#         files=files\n",
    "#     for file in files:\n",
    "#         pdfMiner = PdfMiner(file_path='BDS/'+file)\n",
    "#         print(pdfMiner.convert_pdf_to_txt())\n",
    "#         data.append(pdfMiner.convert_pdf_to_txt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pop(76) # Remving unreadable pdf\n",
    "# Saving data in a text file\n",
    "with open('BDS/data.txt', 'w') as f:\n",
    "    for item in data:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data back from the stored text file\n",
    "file = open(\"BDS/data.txt\",\"r\")\n",
    "data1 = file.read()\n",
    "data = data1.split(\"Next-Paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most common words are:\n",
      " [('learning', 20778), ('model', 13071), ('data', 10762), ('neural', 10330), ('algorithm', 9996), ('using', 9435), ('training', 8853), ('function', 8691), ('also', 7215), ('models', 6728)]\n"
     ]
    }
   ],
   "source": [
    "# Idenifying the top 10 common words\n",
    "comma_replace = [i.replace(',', ' ') for i in data]\n",
    "list_split = [j.split() for j in comma_replace]\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Removing stop words\n",
    "\n",
    "new_list=[]\n",
    "for i in list_split:\n",
    "    new_list += i\n",
    "new_list1 = [x.lower() for x in new_list]\n",
    "new_list2 = [w for w in new_list1 if not w in stop_words] \n",
    "new_list3 = list(filter(lambda word: len(word)> 3, new_list2))\n",
    "\n",
    "word_freq = nltk.FreqDist(new_list3)\n",
    "top_words = word_freq.most_common(10)\n",
    "\n",
    "print(\"The top 10 most common words are:\\n %s\" % top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate probability of a word in a paper\n",
    "def prob(word,l):\n",
    "    count_word = l.count(word)\n",
    "    total_words = len(l)\n",
    "    prob_i = count_word/total_words\n",
    "    return prob_i\n",
    "\n",
    "# Function to calculate the entropy of a random word chosen from a random paper \n",
    "def entropy_word(prob_list):\n",
    "    entropy = -1*np.sum([p*np.log2(p) if p>0 else 0 for p in prob_list])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of the random word - 'also' is : 9.071541330199762\n"
     ]
    }
   ],
   "source": [
    "# Choosing a random word from a random paper and caluclating the entropy of the word\n",
    "random_paper = random.randint(0,len(data))\n",
    "new_list11 = [x.lower() for x in list_split[random_paper]]\n",
    "new_list21 = [w for w in new_list11 if not w in stop_words] \n",
    "new_list31 = list(filter(lambda word: len(word)> 3, new_list21))\n",
    "random_word = new_list31[random.randint(0,len(new_list31))]\n",
    "prob_list = []\n",
    "for i in list_split[:772]:\n",
    "    prob_list.append(prob(random_word,i))\n",
    "entropy = entropy_word(prob_list)\n",
    "print(\"The entropy of the random word - '{}' is : {}\".format(random_word,entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the length of the paragraph : 50\n",
      "The synthesized paragraph (without stop words) using marginal distribution is: \n",
      "\n",
      " case learning international number algorithm using loss section data policy figure different model 2018. matrix used using however given networks proceedings show distribution methods models problem gradient machine method neural network adversarial time based function training model learning results neural also linear conference learning random Ô¨Årst algorithm information performance data\n"
     ]
    }
   ],
   "source": [
    "para_len = int(input(\"Please enter the length of the paragraph : \"))\n",
    "frequent_words = word_freq.most_common(para_len)\n",
    "\n",
    "prob_list1 = {}\n",
    "for i,j in frequent_words:\n",
    "    prob_list1[i] = round((j/len(new_list))*1000)    \n",
    "words = []\n",
    "for index in prob_list1:\n",
    "    for i in range(prob_list1[index]):\n",
    "        words.append(index)\n",
    "para=\"\"\n",
    "for i in range(para_len):\n",
    "    chosen_word = random.choice(words)\n",
    "    para = para+\" \"+chosen_word\n",
    "    words.remove(chosen_word)\n",
    "print(\"The synthesized paragraph (without stop words) using marginal distribution is: \\n\")\n",
    "print(para)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
